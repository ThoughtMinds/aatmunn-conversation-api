from typing_extensions import List, TypedDict
from typing import Optional, AsyncGenerator, Dict
from langchain_core.documents import Document
from api import rag, llm, schema
from langgraph.graph import START, StateGraph
import json
from api.core.logging_config import logger


class State(TypedDict):
    """
    Represents the state of the graph.

    Attributes:
        query (str): The query being asked.
        context (List[Document]): A list of documents retrieved from the vector store.
        navigation (schema.Navigation): The navigation information generated by the model.
    """

    query: str
    context: List[Document]
    navigation: schema.Navigation


def retrieve(state: State):
    """
    Retrieves documents from the vector store based on the query in the state.

    Args:
        state (State): The current state of the graph.

    Returns:
        dict: A dictionary containing the retrieved documents in the 'context' key.
    """
    vectorstore = rag.get_vectorstore()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    retrieved_docs = retriever.invoke(input=state["query"])
    logger.info(f"Retrieved {len(retrieved_docs)} documents")
    return {"context": retrieved_docs}


def generate(state: State):
    """
    Generates a navigation response based on the retrieved context and the query.

    Args:
        state (State): The current state of the graph.

    Returns:
        dict: A dictionary containing the generated navigation information in the 'navigation' key.
    """
    context = {}
    id_mapping = {}
    for i, doc in enumerate(state["context"], start=1):
        id_mapping[i] = doc.id
        context[i] = {
            "id": i,
            "description": doc.page_content,
        }

    logger.info(f"Processing context with {len(context)} documents")
    try:
        chat_model = llm.get_ollama_chat_fallback_model()
        rag_chain = llm.create_chain_for_task(
            task="navigation", llm=chat_model, output_schema=schema.Navigation
        )
        response = rag_chain.invoke(
            {"query": state["query"], "context": json.dumps(context)}
        )
        if response:
            id = response.id
            response.id = id_mapping.get(id, None)
            return {"navigation": response}
    except Exception as e:
        logger.error(f"Failed to get Navigation due to: {e}")
        return {"navigation": schema.Navigation(
            reasoning="Unable to generate navigation response. Please try again.",
            id=None,
        )}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
navigation_graph = graph_builder.compile()


def get_navigation_response(
    query: str, chained: bool = False
) -> Optional[schema.Navigation]:
    """
    Generates a navigation response for a given query using a LangGraph workflow.

    Args:
        query (str): The user's query for navigation.
        chained (bool): This parameter is ignored, but included for compatibility.

    Returns:
        schema.Navigation: The navigation response, or None if no response is generated.
    """
    initial_state = {"query": query}
    result = navigation_graph.invoke(initial_state)
    return result.get("navigation")
